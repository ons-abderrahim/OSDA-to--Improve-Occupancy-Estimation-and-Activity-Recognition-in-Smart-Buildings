{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"__dMIi5tuY-2","executionInfo":{"status":"ok","timestamp":1722690381455,"user_tz":240,"elapsed":15615,"user":{"displayName":"Jawher Dridi","userId":"09677130424740496435"}},"outputId":"7d5c2547-5333-4321-9c9c-6d316dc26da8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/OSDA_Papers_adaptation/OSDA_with_soft_rejection-main/visda17')"],"metadata":{"id":"fLeFApe4Q7uc","executionInfo":{"status":"ok","timestamp":1722690387602,"user_tz":240,"elapsed":929,"user":{"displayName":"Jawher Dridi","userId":"09677130424740496435"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["!python trainer_visda.py"],"metadata":{"id":"hq-pel3NROZC","executionInfo":{"status":"ok","timestamp":1722692665855,"user_tz":240,"elapsed":9720,"user":{"displayName":"Jawher Dridi","userId":"09677130424740496435"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c8c0f37c-c574-41c7-c99b-f86a0ccadd7b"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["(1145, 10)\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","checkpoint/checkpoint\n","/content/drive/.shortcut-targets-by-id/1Hp1PlTDK_RipXxkMpl2OzUFYYDw5fcJG/OSDA_Papers_adaptation/OSDA_with_soft_rejection-main/visda17/trainer_visda.py:179: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  loss_s = criterion_cel(F.softmax(output_s1).index_select(0, ind), target_s.index_select(0, ind))\n","/content/drive/.shortcut-targets-by-id/1Hp1PlTDK_RipXxkMpl2OzUFYYDw5fcJG/OSDA_Papers_adaptation/OSDA_with_soft_rejection-main/visda17/trainer_visda.py:184: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  loss_s += criterion_cel(F.softmax(output_s2).index_select(0, ind), target_s.index_select(0, ind))\n","source acc: 71.4410480349345\n","source acc: 78.77729257641921\n","/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n","Traceback (most recent call last):\n","  File \"/content/drive/.shortcut-targets-by-id/1Hp1PlTDK_RipXxkMpl2OzUFYYDw5fcJG/OSDA_Papers_adaptation/OSDA_with_soft_rejection-main/visda17/trainer_visda.py\", line 497, in <module>\n","    train(i)\n","  File \"/content/drive/.shortcut-targets-by-id/1Hp1PlTDK_RipXxkMpl2OzUFYYDw5fcJG/OSDA_Papers_adaptation/OSDA_with_soft_rejection-main/visda17/trainer_visda.py\", line 305, in train\n","    lb = criterion_bcewithlogits(F.softmax(output_t1, dim=1).mean(dim=0).unsqueeze(dim=0), torch.tensor([1/(num_class-1)]*(num_class-1) ).unsqueeze(dim=0).cuda())\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/drive/.shortcut-targets-by-id/1Hp1PlTDK_RipXxkMpl2OzUFYYDw5fcJG/OSDA_Papers_adaptation/OSDA_with_soft_rejection-main/visda17/trainer_visda.py\", line 73, in forward\n","    loss = F.binary_cross_entropy_with_logits(pred_score, target_score, size_average=True)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 3224, in binary_cross_entropy_with_logits\n","    raise ValueError(f\"Target size ({target.size()}) must be the same as input size ({input.size()})\")\n","ValueError: Target size (torch.Size([1, 1])) must be the same as input size (torch.Size([1, 2]))\n"]}]}]}